=======================
Hail Query Call Caching
=======================

.. author:: Edmund Higham <edhigham@gmail.com>
.. date-accepted:: Leave blank. This will be filled in when the proposal is accepted.
.. implemented:: Leave blank. This will be filled in with the first Hail version which
                 implements the described feature.
.. header:: This proposal is `discussed at this pull request <https://github.com/hail-is/hail-rfc/pull/0>`_.
            **After creating the pull request, edit this file again, update the
            number in the link, and delete this bold sentence.**
.. sectnum::
.. contents::
.. role:: scala(code)

.. Here you should write a short abstract motivating and briefly summarizing the
.. proposed change.

Motivation
==========
`gnomAD <https://gnomad.broadinstitute.org/>`_ is one project that uses hail
query for various analysis and quality-control pipelines which can cost in
excess of [DURATION] and [DOLLARS] to compute.
Evaluating a query reduces the results of many sub- queries with another to
combine their results; a single failure in any of these will result in the whole
query failing.
These failures can happen for a multitude of reasons - from programmer error to
general cloud instability.
Sometimes, the only way to fix a failure is to re-run it.
Without an efficient means of re-trying queries and computational demand ever
increasing, hail may become prohibitively expensive for such projects.

To reduce the cost of re-tries, Hail experimented with the notion of "checkpoint
files" when writing a :scala:`MatrixTable` to persistent storage.
A "checkpoint file" was a user-defined location where Hail accumulated metadata
about the partitions it had already computed and written.
Then, if a failure happened and the query was re-run, Hail would use these
metadata to only re-run those computations that had not previously succeeded,
accumulating the metadata from the successful runs into the checkpoint file.
This worked because query operations are (ostensibly) *referentially-transparent*,
ie. evaluation of an operation can be safely replaced with its value without
changing the meaning of the query.
There were a number of problems this design, including

* it only applied to writing :scala:`MatrixTable` and was not generalised to
  other kinds of hail expression (such as accumulations) or data types.
* the user was required to keep track of these staging files and guarantee that
  the :scala:`MatrixTable` contained therein was the same as that used in the
  query.
* it only applied to one write operation - queries could fail before they
  reached that write, in which case retrying would start from the beginning.

As part of Hail's continued effort to implement query execution on Hail batch,
`#12879 <https://github.com/hail-is/hail/pull/12879>`_ removed support for this
experimental feature for simplicity, presenting the opportunity to improve upon
checkpoint files.
We thus needs a means of resuming execution from where the last query left off.

Proposed Change Specification
=============================

We propose to implement fast-restarts via "call-caching", a generalisation of
checkpoint files that acts on Hail query's intermediate representation
(:scala:`IR`), rather than higher-level abstractions exposed to the python DSL.
Experience tells us that the majority of time spent on expensive and
scientifically interesting queries is within the tasks generated by
:scala:`CollectDistributedArray` (:scala:`CDA`).
This is because many of the table operations in query's :scala:`IR` are lowered
into one or more :scala:`CDA` operations.
Consequently, we focus our attention on caching the intermediate results of
these tasks.
This has the benefit of not only fulfilling the purpose of checkpoint files for
:scala:`MatrixWrite` operations, but also any other operation that is lowered
in terms of :scala:`CDA`.

:scala:`CDA` can be thought of as a distributed map-reduce operation, from some
input "context" for each partition in a table (eg, the path to the file
where the partition is serialised), a computation on that partition, and some
combiner for the results of those computations.
For what follows, let an *activation* be a particular invocation of a
:scala:`CDA` pipeline (implemented via :scala:`collectDArray`).

At a high-level, when the driver performs an *activation*, it will look in its
*execution cache* to see if it had successfully performed that *activation*
in the past.
The *cache* contains the results for all the successful partition computations.
The driver compares the tasks for each partition with the results in the cache
and removes those tasks that have already been completed.
It then executes any remaining work and updates the execution cache with their
results.
If all the work completes successfully, the driver returns the now-cached
results to be used in the the rest of the query.
The driver will cache the results of successful *activations* only.
Failed *activations* (ie. those that errored) will be handled in the usual way,
potentially failing the query.

We require two things to determine if the driver had successfully executed an
operation:

1. a way of looking up *activations* in a *cache*, and
2. then design of the execution cache itself

Semantic Hashing
----------------
To lookup operations in the cache, we need a way of producing an identifier
that uniquely represents the value produced by an *activation*.
We do this by defining a *semantic hash* for the activation, comprised of:

a) a *static* component computed from the :scala:`IR` that generated the
   operation
b) a *dynamic* component for the particular activation instance.

For most :scala:`IR` nodes, the *static* component can be computed purely from
their inputs plus some contribution uniquely representing the semantics of that
class of :scala:`IR`.
For :scala:`IR` nodes that read external files, we have to be a little more
cautious and ensure that those files haven't changed since we last read them.
Thus, we need to include some kind of checksum or digest of that file.
This static component can be passed down the lowering pipeline to the code
generator and driver, which, when performing an activation, can mix the static
component with a dynamically generated activation id to form the semantic hash.

Execution Cache
---------------

Users will "bring their own"\ :sup:`TM` cache directory where cached
computations will be stored.
This cache dir will be an prefix in local or cloud storage.
The driver will store cache files named ``{cachedir}/{hail-version}/{semhash}``.
These files will contain accumulated activation results, indexed by their
partition number.


> Discussion

* modify etag on cache objects when reading so their lifecycle policy is reset

etags don't seem to reset objects' retention though using an holds in
conjunction with bucket retention policies could!

cache thunks that validate files
clean cache on successful pipeline completion?

Examples
========

To opt in or out of fast-restarts, users will set hail flags in their python
client:

..  code-block:: python

    >> hl._set_flags(fast_restarts=True)
    >> hl._set_flags(cachedir='gs://my-bucket/cache/0')


Effect and Interactions
=======================
Your proposed change addresses the issues raised in the motivation. Explain how.

Also, discuss possibly contentious interactions with existing language or compiler
features. Complete this section with potential interactions raised
during the PR discussion.

Costs and Drawbacks
===================

.. Give an estimate on development and maintenance costs. List how this affects
.. learnability of the language for novice users. Define and list any remaining
.. drawbacks that cannot be resolved.

* Only cache around :scala:`CollectDistributedArray`
* Caching requires overhead from lookups and insertions
* Not completely hidden from user - requires much diligent error handling
* Requires that we start from the beginning until we get a cache-miss.

  * A more efficient fast-restart mechanism might search for the first
    cache-hit from the end of the query.

Alternatives
============

.. List alternative designs to your proposed change. Both existing
.. workarounds, or alternative choices for the changes. Explain
.. the reasons for choosing the proposed change over these alternative:
.. *e.g.* they can be cheaper but insufficient, or better but too
.. expensive. Or something else.

.. The PR discussion often raises other potential designs, and they should be
.. added to this section. Similarly, if the proposed change
.. specification changes significantly, the old one should be listed in
.. this section.

* Adopt a graph-reduction execution model?
* Using a key-value store + persistent storage for a cache to reduce lookup
  latency

Unresolved Questions
====================

Execution Cache
---------------

* How long should the cache live?

Presumably as long as tmpdir as the files it caches reside in tmpdir

* Where do we write

Configurable and user defined. We'll likely default to the tempdir unless
a user specifies otherwise.


* Who do we handle multiple processes executing the same query?
  - atomic writes, via db or file re-writes
  - one wins, doesn't matter which
* Should users "bring-their-own"\ :sup:`TM` cache?

Implementation Plan
===================

The reader should note that implementation examples below are for illustrative
purposes only and that the real implementation may differ.

Semantic Hashes
---------------

Computing Static Component
^^^^^^^^^^^^^^^^^^^^^^^^^^

We can compute the static component of a semantic hash from a bottom-up
traversal of the IR ``IR``.
Since the ``IR`` supports references, we need to compute a binding environment
top-down that maps names to their definitions, so we can look up the static
component of the value being referenced:

..  code-block:: scala

    type BindingEnv = Map[String, BaseIR]

    object FlattenTopDown {
      def apply(ir: BaseIR, env: BindingEnv): Iterator[(BaseIR, BindingEnv)] =
        ir match {
          case Let(name, value, body) =>
            FlattenTopDown(value, env) ++
            FlattenTopDown(body, env.put(name, value)) ++
            Iterator.single(ir, env)

          case ... =>
        }
    }

Then, assuming we have an appropriate hashing algorithm and a way of combining
hashes:

..  code-block:: scala

    def hash(a: Any): Hash = ???
    @newtype case class Hash(v: ???) {
      def <>(b: Hash): Hash = ???
    }

Then:

..  code-block:: scala

    object BottomUp {
      def apply(fs: FS, memo: Memo[Hash])(ir: BaseIR, env: BindingEnv): Hash =
        ir match {
          case Ref(name, _) =>
            hash(classOf[Ref]) <> memo(env(name))

          case TableRead(_, _, reader) =>
            reader
              .pathsUsed
              .map(fs.digest)
              .foldLeft(hash(classOf[TableRead]))(_ <> hash(_))

          case ir if DependsOnlyOnInputs(ir) =>
            ir.children.foldLeft(hash(ir.getClass))(_ <> memo(_))

          case ... =>
        }
    }

Computing Dynamic Component
^^^^^^^^^^^^^^^^^^^^^^^^^^^

In ``Emit.scala``, pass down the memoized static components of the semantic
hash.
When emitting :scala:`CollectDistributedArray`, combine the dynamic id with the
static component of the hash and pass that to :scala:`collectDArray`.


Execution Cache
---------------

Given an interface for an :scala:`ExecutionCache`` of the form:

..  code-block:: scala

    trait ExecutionCache {
        def lookup(h: SemanticHash): Array[(Int, Array[Byte])]
        def put(h: SemanticHash, res: Array[(Int, Array[Byte])]): Unit
    }

We can implement a file-system cache that uses a file prefix plus the current
version of Hail to generate a "root" directory, under which all cache files are
stored by their semantic hash.
An implementation might look as follows:

..  code-block:: scala

    final case class FSExecutionCache(fs: FS, cachedir: String)
      extends ExecutionCache {

      override def lookup(h: SemanticHash): Array[(Int, Array[Byte])] =
        Using(fs.open(s"$cachedir/${HailContext.version}/$h")) { readlines }
          .getOrElse(Array.empty)

      override def put(h: SemanticHash, res: Array[(Int, Array[Byte])]): Unit =
        fs.write(s"$cachedir/${HailContext.version}/$h") { ostream =>
          res.foreach { case (index, bytes) =>
            ostream.write(index)
            ostream.write(", ")
            ostream.write(bytes)
            ostream.write("\n")
          }
        }
    }

For testing, we can simply create a wrapper around a :scala:`mutable.HashMap`:

..  code-block:: scala

    @newtype case class MemExecutionCache(
        m: mutable.HashMap[SemanticHash, Array[(Int, Array[Byte])]]
    ) extends ExecutionCache {
        ...
    }

Endorsements
=============

.. (Optional) This section provides an opportunity for any third parties to express their
.. support for the proposal, and to say why they would like to see it adopted.
.. It is not mandatory for have any endorsements at all, but the more substantial
.. the proposal is, the more desirable it is to offer evidence that there is
.. significant demand from the community.  This section is one way to provide
.. such evidence.
